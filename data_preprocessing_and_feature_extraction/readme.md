# Data preprocessing and feature extraction

In this folder the necessary information for feature generation is summarized. The features need to be saved into the corresponding folder with it's name, as showed in the `datasets` folder.

## Data preprocessing
As video quality varies among different samples to a large extent, we first discarded some samples. Specifically, videos were omitted from consideration in cases if the OpenFace failed to capture facial landmarks continuously for ten consecutive frames, which may occur due to situations like accidental face scratching. Additionally, if the framework failed to recognize the face for a total duration exceeding thirty frames, it was also excluded. In instances where the face was unrecognizable for fewer than ten frames, we applied a corrective measure by discarding the problematic frames and substituting them with replicating the last successfully identified frame. Face recognition might falter due to various factors, such as participants tilting their heads, moving outside the video frame, or positioning in a manner where only a segment of their face is visible (like just the forehead). Additionally, inadequate lighting on their faces can also hinder recognition. After these data preprocessing steps, 6 and 12 participants with 19 and 48 positive and 42 and 111 negative labels are removed from the EngageNet and Colorado datasets, respectively.

To have comparable results with a trained MLP model, we simplified the dimension of the input feature vector by averaging every ten features corresponding to single frames into one and cutting the last four frames. The input dimension was changed from $124 \times 520$ to $12 \times 520$. As a next step, we flattened this feature matrix into a vector.

## Features
Consistent with numerous related works, issues of self-regulation have been linked to facial expressions, eye gaze patterns, and human emotions. Given this association, we extracted these features for the classification of all our video samples. Instead of using raw pixel values from each frame, we emphasized the significance of features by leveraging those extracted by trained neural networks or from facial landmark coordinates. This approach not only simplifies our model's input but also underscores the pivotal role of feature quality in detecting self-regulation problems. Hence, evaluating the feature extractor becomes crucial whenever feasible. Many previous works use features generated by the OpenFace frameworks, which are specialized for facial behavior and eye tracking and retrieves explicit information like facial landmarks, facial action units, head pose and gaze direction in a vector of dimension 709. Since the predicted facial landmark coordinates are measurable in contrast to extracted feature vectors from a neural network, the quality of the predicted points can be evaluated on datasets containing manual labels describing facial landmark coordinates. An example of such a dataset is the [Eyeblink8](https://link.springer.com/chapter/10.1007/978-3-319-16199-0_31) dataset, which contains ground truth eye corner coordinates. Since the correct detection of the eyes plays a crucial role in self-regulation problem detection, we only focus on evaluating these predicted coordinates by OpenFace.

In the Eyeblink8 dataset, the eye corner coordinates are labeled, which makes it possible to calculate the distance between the eye corner coordinates predicted by OpenFace and the given ones. This dataset contains students sitting at home, acting naturally, in the same setup as the four datasets used in experiments. The Eyeblink8 dataset contains 8 videos of 4 individuals recorded by web cameras (resolution: $640\times 480$). For each frame, the left and right eye coordinates are given pixel-wise. In the table below, the quality of OpenFace feature extraction is provided. The distance between the predicted 4 eye coordinates and the ground truth (GT) coordinates is compared on each frame. Overall, the confidence of the feature extraction for the whole video provided by OpenFace is above 0.97 for all videos, which generally would point to successful feature extraction on almost all frames. On the other hand, three out of eight videos contain more than 200 incorrect predictions (more than a ten-pixel difference between the predicted and ground truth of the coordinates), while the remaining only consist of almost perfect predictions. This suggests that although the extracted OpenFace features have a decent quality in general, for some participants, it can make incorrect predictions without a drop in its confidence. Therefore, combining the extracted OpenFace features with others is necessary.
<table class="tg">
<thead>
  <tr>
    <th class="tg-c3ow"></th>
    <th class="tg-c3ow" colspan="2">Client 1</th>
    <th class="tg-c3ow" colspan="2">Client 2</th>
    <th class="tg-c3ow" colspan="2">Client 3</th>
    <th class="tg-c3ow" colspan="2">Client 4</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">Video 1</td>
    <td class="tg-c3ow">Video 2</td>
    <td class="tg-c3ow">Video 1</td>
    <td class="tg-c3ow">Video 2</td>
    <td class="tg-c3ow">Video 1</td>
    <td class="tg-c3ow">Video 2</td>
    <td class="tg-c3ow">Video 1</td>
    <td class="tg-c3ow">Video 2</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Glasses?</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">no</td>
    <td class="tg-c3ow">yes</td>
  </tr>
  <tr>
    <td class="tg-c3ow">abs. dist. from GT</td>
    <td class="tg-c3ow">2.99</td>
    <td class="tg-c3ow">3.38</td>
    <td class="tg-c3ow">2.56</td>
    <td class="tg-c3ow">1.14</td>
    <td class="tg-c3ow">1.01</td>
    <td class="tg-c3ow">0.75</td>
    <td class="tg-c3ow">1.04</td>
    <td class="tg-c3ow">1.18</td>
  </tr>
  <tr>
    <td class="tg-c3ow"># frams with dist. &gt; 10</td>
    <td class="tg-c3ow">397.25</td>
    <td class="tg-c3ow">362.63</td>
    <td class="tg-c3ow">215.75</td>
    <td class="tg-c3ow">13.50</td>
    <td class="tg-c3ow">4.25</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
  </tr>
  <tr>
    <td class="tg-c3ow"># frams with dist. &gt; 20</td>
    <td class="tg-c3ow">75.75</td>
    <td class="tg-c3ow">123.88</td>
    <td class="tg-c3ow">74.00</td>
    <td class="tg-c3ow">3.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
  </tr>
  <tr>
    <td class="tg-c3ow"># frams with dist. &gt; 50</td>
    <td class="tg-c3ow">4.25</td>
    <td class="tg-c3ow">7.50</td>
    <td class="tg-c3ow">1.50</td>
    <td class="tg-c3ow">3.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
    <td class="tg-c3ow">0.00</td>
  </tr>
  <tr>
    <td class="tg-c3ow"># frams</td>
    <td class="tg-c3ow">15711</td>
    <td class="tg-c3ow">11123</td>
    <td class="tg-c3ow">9216</td>
    <td class="tg-c3ow">5315</td>
    <td class="tg-c3ow">10663</td>
    <td class="tg-c3ow">5060</td>
    <td class="tg-c3ow">9014</td>
    <td class="tg-c3ow">4890</td>
  </tr>
  <tr>
    <td class="tg-c3ow">avg. OpenFace conf.</td>
    <td class="tg-c3ow">0.978</td>
    <td class="tg-c3ow">0.979</td>
    <td class="tg-c3ow">0.979</td>
    <td class="tg-c3ow">0.974</td>
    <td class="tg-c3ow">0.980</td>
    <td class="tg-c3ow">0.980</td>
    <td class="tg-c3ow">0.980</td>
    <td class="tg-c3ow">0.972</td>
  </tr>
</tbody>
</table>

Since the quality of the extracted OpenFace features can vary and a previous study showed the increased performance and potential for generalization using latent transfer-learned emotion features for mind wandering detection, we employed another publicly available network, EmoNet, to extract features pointing to emotions from video frames and to have a second independent input. EmoNet was designed for emotion classification and extracting implicit hidden vectors of length 512. Trained models are publicly available, therefore, the feature vectors are easily reproducible. We used [the model designed to recognize five emotions](https://github.com/face-analysis/emonet). 

In the next, we tried different combinations of the EmoNet features and the gaze features (i.e., six gaze vector coordinates and two gaze angles) from OpenFace as an input of the neural network in all later experiments. The features were combined in form of vector concatenation. To test the robustness and sensitivity of the centralized and decentralized frameworks to different feature sets, we conducted a series of experiments, where we selected a variety of input features. The same strategies for learning rate search were applied as in the previous section, resulting in the values in the table below. 
<table class="tg">
<thead>
  <tr>
    <th class="tg-c3ow">Features</th>
    <th class="tg-c3ow">Algorithm</th>
    <th class="tg-c3ow">Colorado</th>
    <th class="tg-c3ow">Korea</th>
    <th class="tg-c3ow">EngageNet</th>
    <th class="tg-c3ow">DAiSEE</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-9wq8" rowspan="2">EmoNet</td>
    <td class="tg-c3ow">non-FL</td>
    <td class="tg-c3ow">1e-4</td>
    <td class="tg-c3ow">1e-2</td>
    <td class="tg-c3ow">1e-4</td>
    <td class="tg-c3ow">1e-4</td>
  </tr>
  <tr>
    <td class="tg-c3ow">FedAvg</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-1.5</td>
    <td class="tg-c3ow">1e-3.5</td>
    <td class="tg-c3ow">1e-3</td>
  </tr>
  <tr>
    <td class="tg-9wq8" rowspan="2">OpenFace</td>
    <td class="tg-c3ow">non-FL</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-4</td>
    <td class="tg-c3ow">1e-4</td>
  </tr>
  <tr>
    <td class="tg-c3ow">FedAvg</td>
    <td class="tg-c3ow">1e-2.5</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-3.5</td>
    <td class="tg-c3ow">1e-3</td>
  </tr>
  <tr>
    <td class="tg-9wq8" rowspan="2">OpenFace<br> gaze</td>
    <td class="tg-c3ow">non-FL</td>
    <td class="tg-c3ow">1e-2</td>
    <td class="tg-c3ow">1e-1</td>
    <td class="tg-c3ow">1e-1</td>
    <td class="tg-c3ow">1e-2</td>
  </tr>
  <tr>
    <td class="tg-c3ow">FedAvg</td>
    <td class="tg-c3ow">1e-1</td>
    <td class="tg-c3ow">1e-1</td>
    <td class="tg-c3ow">1e-1</td>
    <td class="tg-c3ow">1e-2</td>
  </tr>
  <tr>
    <td class="tg-9wq8" rowspan="2">EmoNet + <br>OpenFace</td>
    <td class="tg-c3ow">non-FL</td>
    <td class="tg-c3ow">1e-4</td>
    <td class="tg-c3ow">1e-2</td>
    <td class="tg-c3ow">1e-4</td>
    <td class="tg-c3ow">1e-4</td>
  </tr>
  <tr>
    <td class="tg-c3ow">FedAvg</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-3</td>
    <td class="tg-c3ow">1e-3.5</td>
    <td class="tg-c3ow">1e-2.5</td>
  </tr>
</tbody>
</table>

The experiment results are benchmarked in the following table. In almost all cases, FedAvg achieved better performance compared to a centralized setting trained on the same feature sets. Models trained only on the OpenFace eight gaze features resulted in high model performance. However, upon closer examination, it became apparent that the model failed to learn effectively, as indicated by a training MCC score (Matthews Correlation Coefficient) close to zero. Consequently, it can be inferred that these models predicted the majority class, like the MLP baseline. By combining all features from the EmoNet and OpenFace, the input dimension of each sample reached $124 \times 1221$, which increased the computational time and overloaded the model, and hence resulted in decreased model performance. When the input features are only relying on encoded emotional information (only Emonet features), in both settings, the model performances are worse compared to other models trained on facial landmarks (i.e., OpenFace features). Overall, the model performances in centralized and decentralized settings have only small oscillations (excluding the model trained on the OpenFace features), underscoring the robustness and generalizability of our models. In addition, increasing the features retrieved by OpenFace did not substantially improve performance. As a final decision, we fixed our feature portfolio to EmoNet feature and OpenFace gaze feature, which are of dimension 520.
<table class="tg">
<thead>
  <tr>
    <th class="tg-nrix">Features</th>
    <th class="tg-nrix" colspan="2">Performance</th>
    <th class="tg-nrix">Colorado</th>
    <th class="tg-nrix">Korea</th>
    <th class="tg-nrix">EngageNet</th>
    <th class="tg-nrix">DAiSEE</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix" rowspan="4">EmoNet</td>
    <td class="tg-nrix" rowspan="2">non-FL</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">38.8±11.1</td>
    <td class="tg-baqh">78.2±8.7</td>
    <td class="tg-baqh">65.1±4.1</td>
    <td class="tg-baqh">56.1±7.4</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">41.2±7.7</td>
    <td class="tg-baqh">71.3±12.5</td>
    <td class="tg-baqh">66.1±4.5</td>
    <td class="tg-baqh">49.5±7.1</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="2">FedAvg</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">64.7±4.0</td>
    <td class="tg-baqh">82.5±5.0</td>
    <td class="tg-baqh">64.3±2.3</td>
    <td class="tg-baqh">56.7±8.7</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">65.2±4.7</td>
    <td class="tg-baqh">78.1±8.7</td>
    <td class="tg-baqh">64.3±2.7</td>
    <td class="tg-baqh">50.0±9.0</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="4">OpenFace</td>
    <td class="tg-nrix" rowspan="2">non-FL</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">47.1±7.0</td>
    <td class="tg-baqh">16.2±18.3</td>
    <td class="tg-baqh">73.8±4.5</td>
    <td class="tg-baqh">64.0±3.9</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">45.6±6.0</td>
    <td class="tg-baqh">15.6±12.3</td>
    <td class="tg-baqh">74.2±4.6</td>
    <td class="tg-baqh">58.0±4.8</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="2">FedAvg</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">59.8±4.1</td>
    <td class="tg-baqh">59.8±13.4</td>
    <td class="tg-baqh">68.5±4.9</td>
    <td class="tg-baqh">67.1±5.2</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">63.6±7.0</td>
    <td class="tg-baqh">49.6±13.2</td>
    <td class="tg-baqh">68.2±4.8</td>
    <td class="tg-baqh">62.4±6.9</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="4">OpenFace<br> gaze</td>
    <td class="tg-nrix" rowspan="2">non-FL</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">56.0±18.0</td>
    <td class="tg-baqh">69.2±24.6</td>
    <td class="tg-baqh">78.7±1.1</td>
    <td class="tg-baqh">63.7±19.9</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">62.1±17.1</td>
    <td class="tg-baqh">63.9±26.6</td>
    <td class="tg-baqh">78.6±1.1</td>
    <td class="tg-baqh">62.4±21.0</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="2">FedAvg</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">71.7±0.4</td>
    <td class="tg-baqh">81.0±12.6</td>
    <td class="tg-baqh">75.3±1.1</td>
    <td class="tg-baqh">75.6±7.9</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">72.7±1.1</td>
    <td class="tg-baqh">77.6±18.1</td>
    <td class="tg-baqh">76.5±0.8</td>
    <td class="tg-baqh">78.2±13.4</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="4">EmoNet + <br>OpenFace<br>gaze</td>
    <td class="tg-nrix" rowspan="2">non-FL</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">40.4±12.0</td>
    <td class="tg-baqh">77.6±10.6</td>
    <td class="tg-baqh">62.0±2.4</td>
    <td class="tg-baqh">57.5±6.7</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">42.4±9.7</td>
    <td class="tg-baqh">71.3±14.9</td>
    <td class="tg-baqh">62.5±2.8</td>
    <td class="tg-baqh">50.7±7.1</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="2">FedAvg</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">64.9±3.9</td>
    <td class="tg-baqh">83.6±4.1</td>
    <td class="tg-baqh">66.6±5.3</td>
    <td class="tg-baqh">53.6±6.4</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">63.8±4.5</td>
    <td class="tg-baqh">79.7±6.7</td>
    <td class="tg-baqh">67.1±5.7</td>
    <td class="tg-baqh">46.8±4.0</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="4">EmoNet + <br>OpenFace</td>
    <td class="tg-nrix" rowspan="2">non-FL</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">43.2±12.9</td>
    <td class="tg-baqh">68.5±10.6</td>
    <td class="tg-baqh">66.3±3.1</td>
    <td class="tg-baqh">57.0±4.4</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">44.6±9.9</td>
    <td class="tg-baqh">58.9±11.9</td>
    <td class="tg-baqh">68.1±3.4</td>
    <td class="tg-baqh">50.0±4.4</td>
  </tr>
  <tr>
    <td class="tg-nrix" rowspan="2">FedAvg</td>
    <td class="tg-nrix">F1[%]</td>
    <td class="tg-nrix">61.9±1.5</td>
    <td class="tg-baqh">80.6±7.9</td>
    <td class="tg-baqh">65.1±2.6</td>
    <td class="tg-baqh">61.1±16.1</td>
  </tr>
  <tr>
    <td class="tg-nrix">Acc[%]</td>
    <td class="tg-nrix">61.9±2.3</td>
    <td class="tg-baqh">75.6±11.7</td>
    <td class="tg-baqh">66.6±3.7</td>
    <td class="tg-baqh">57.3±18.2</td>
  </tr>
</tbody>
</table>

## EmoNet
The necessary files are in the emonet_feature_extraction folder. Te source of the pre-trained model and the code is available at: https://github.com/face-analysis/emonet.

## OpenFace
The OpenFace features can be generated by downloading their repository: https://github.com/TadasBaltrusaitis/OpenFace and following the instructions. For videos, `FeatureExtraction.exe` needs to be run (`FeatureExtraction.exe -f  path2video`).

## MeGlass
The dataset is available at: https://github.com/cleardusk/MeGlass. Since no pre-trained model was published, a ResNet 18 was trained on their dataset and used for feature extraction.
